{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nT73ZMWo_ioo"
   },
   "source": [
    "### Semantic Control in Diffusion Inpainting: Merging RePaint Sampling with Text-Driven Generation in GLIDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T19:33:06.002520Z",
     "start_time": "2025-03-10T19:33:02.178855Z"
    },
    "executionInfo": {
     "elapsed": 45,
     "status": "ok",
     "timestamp": 1741669902965,
     "user": {
      "displayName": "Arnauld LINVANI",
      "userId": "15767780697894810406"
     },
     "user_tz": -60
    },
    "id": "kE5Lr8UJ_iow"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/infres/kbrowder-24/jp/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "from typing import Tuple\n",
    "import numpy as np\n",
    "import torch as torch\n",
    "import torch.nn.functional as F\n",
    "import gradio as gr\n",
    "\n",
    "from glide_text2im.download import load_checkpoint\n",
    "from glide_text2im.model_creation import (\n",
    "    create_model_and_diffusion,\n",
    "    model_and_diffusion_defaults,\n",
    "    model_and_diffusion_defaults_upsampler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1741669902970,
     "user": {
      "displayName": "Arnauld LINVANI",
      "userId": "15767780697894810406"
     },
     "user_tz": -60
    },
    "id": "rDU6kLusYHVP",
    "outputId": "f6ad2c8b-3a63-48a3-adc6-7a18274c90ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.20.1\n"
     ]
    }
   ],
   "source": [
    "print(gr.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.6\n"
     ]
    }
   ],
   "source": [
    "import pydantic\n",
    "print(pydantic.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T19:34:58.443918Z",
     "start_time": "2025-03-10T19:34:58.441481Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1741669902973,
     "user": {
      "displayName": "Arnauld LINVANI",
      "userId": "15767780697894810406"
     },
     "user_tz": -60
    },
    "id": "fADocbM8_iox",
    "outputId": "afef2dd4-7668-4cb8-b22a-442fc3572900"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:2\n"
     ]
    }
   ],
   "source": [
    "# This notebook supports both CPU and GPU.\n",
    "# On CPU, generating one sample may take on the order of 20 minutes.\n",
    "# On a GPU, it should be under a minute.\n",
    "\n",
    "has_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda:2' if has_cuda else 'cpu')\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oZZANjle_iox"
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yAB7vrfV__Xm"
   },
   "source": [
    "For practical reasons, we run the notebook in the Google Drive folder where it is located. We need to provide a path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2081,
     "status": "ok",
     "timestamp": 1741669905055,
     "user": {
      "displayName": "Arnauld LINVANI",
      "userId": "15767780697894810406"
     },
     "user_tz": -60
    },
    "id": "6EzoM6qKABRD",
    "outputId": "bd01d8f4-884e-4c38-8ade-a62813d9b2a3"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# # Replace path_to_notebook with your actual path\n",
    "# path_to_notebook = \"drive/MyDrive/Colab Notebooks/M1 Data AI/Computer Vision/Project\"\n",
    "\n",
    "# %cd {path_to_notebook}\n",
    "\n",
    "# !pwd\n",
    "# !ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rUvvEdHO_ioy"
   },
   "source": [
    "### Sampling with Repaint sampling method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1741669905133,
     "user": {
      "displayName": "Arnauld LINVANI",
      "userId": "15767780697894810406"
     },
     "user_tz": -60
    },
    "id": "x9v_KcWj_io1"
   },
   "outputs": [],
   "source": [
    "def process_input_with_mask(input_with_mask, prompt, guidance_scale):\n",
    "    \"\"\"\n",
    "    Process an image with drawing mask and extract both components for inpainting.\n",
    "\n",
    "    Args:\n",
    "        input_with_mask: Image with alpha channel containing mask information\n",
    "        prompt: Text prompt for guiding the inpainting\n",
    "        guidance_scale: Scale factor for classifier-free guidance\n",
    "\n",
    "    Returns:\n",
    "        Inpainted image or error message\n",
    "    \"\"\"\n",
    "    # return \"function executed\"\n",
    "    gr.Info(f\"Input Image Dict: {input_with_mask.keys()}\")\n",
    "    gr.Info(f\"Background: {input_with_mask['background'].shape if isinstance(input_with_mask['background'], np.ndarray) else 'Invalid Image'}\")\n",
    "    gr.Info(f\"Layers: {len(input_with_mask['layers'])}\")\n",
    "    gr.Info(f\"Composite: {input_with_mask['composite'].shape if isinstance(input_with_mask['composite'], np.ndarray) else 'Invalid Image'}\")\n",
    "    gr.Info(f\"Layer 1: {input_with_mask['layers'][0].shape if isinstance(input_with_mask['layers'][0], np.ndarray) else 'Invalid Image'}\")\n",
    "    # return input_with_mask['layers'][0]\n",
    "\n",
    "\n",
    "    # Check if there's any drawing on the image\n",
    "    if input_with_mask is None:\n",
    "        return \"Please upload an image first.\"\n",
    "    \n",
    "\n",
    "    if len(input_with_mask['background'].shape) == 3:\n",
    "        if len(input_with_mask['layers']) == 1:\n",
    "            # The 4th channel contains the mask (alpha channel)\n",
    "            original_image = input_with_mask['background'][:, :, :3]\n",
    "\n",
    "            # Extract mask from alpha channel, white (255) marks areas to inpaint\n",
    "            mask = input_with_mask['layers'][0]\n",
    "\n",
    "            # Check if any masking was done\n",
    "            if np.max(mask) == 0:\n",
    "                return \"Please draw on the image to create a mask for inpainting.\"\n",
    "\n",
    "            # Process the image and mask for inpainting\n",
    "            gr.Info(f\"Original Image: {original_image.shape}\")\n",
    "            gr.Info(f\"Mask: {mask.shape}\")\n",
    "            return inpaint_image(original_image, mask, prompt, guidance_scale, size=64)\n",
    "        else:\n",
    "            return \"The image doesn't have an alpha channel for masking. Please draw on the image.\"\n",
    "    else:\n",
    "        return \"Please upload a valid image and draw on it to create a mask.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import repaint_sampling as RS\n",
    "import repaint_patcher as RP\n",
    "import prepare_glide_inpaint as PGI\n",
    "from image_util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1741669905160,
     "user": {
      "displayName": "Arnauld LINVANI",
      "userId": "15767780697894810406"
     },
     "user_tz": -60
    },
    "id": "aRRHJjb4_io1"
   },
   "outputs": [],
   "source": [
    "def inpaint_image(input_image, mask_image, prompt, guidance_scale=7.0, up_sample_model=False, size=256, batch_size=1):\n",
    "    \"\"\"Inpaint the masked region of the image based on the text prompt.\"\"\"\n",
    "    model, diffusion, options = PGI.create_glide_generative(device=device, cuda=has_cuda)\n",
    "    RP.patch_model_for_repaint(diffusion)\n",
    "    sampler = RS.CFGSamplerRepaint(model, diffusion, options, guidance_scale, device)\n",
    "\n",
    "    # Process input image\n",
    "    image_tensor = process_image(input_image, size=size)\n",
    "\n",
    "    # Handling the mask from the drawing canvas\n",
    "    if len(mask_image.shape) == 3 and mask_image.shape[2] == 4:\n",
    "        mask_array = mask_image[:, :, 3]\n",
    "    elif len(mask_image.shape) == 2:\n",
    "        mask_array = mask_image\n",
    "    else:\n",
    "        mask_array = np.mean(mask_image, axis=2) if mask_image.shape[2] == 3 else mask_image\n",
    "\n",
    "    mask_tensor = process_mask(mask_array, size=size)\n",
    "\n",
    "    # In GLIDE's inpainting model, 0 means \"inpaint this\", 1 means \"keep this\"\n",
    "    # But in our UI, white (255) means \"inpaint this\", so we invert the mask\n",
    "    inpaint_mask = 1.0 - mask_tensor\n",
    "\n",
    "    # Rest of your function remains the same\n",
    "    # For RePaint, we need the ground truth image and keep mask\n",
    "    gt = image_tensor\n",
    "    gt_keep_mask = inpaint_mask  # 1 for areas to keep (not inpaint)\n",
    "\n",
    "    # Sampling from the model\n",
    "    print(\"Generating inpainted image...\")\n",
    "\n",
    "    if not up_sample_model:\n",
    "        jump_params = {\n",
    "            \"t_T\": 250,\n",
    "            \"n_sample\": 1,\n",
    "            \"jump_length\": 10,\n",
    "            \"jump_n_sample\": 10\n",
    "        }\n",
    "        samples = sampler.sample(gt, gt_keep_mask, prompt, batch_size, jump_params=jump_params)\n",
    "\n",
    "        # Convert the tensor to a numpy array in the range [0, 255]\n",
    "        samples = ((samples + 1) * 127.5).clamp(0, 255).to(torch.uint8)\n",
    "        samples = samples.permute(0, 2, 3, 1).contiguous()\n",
    "        sample_image = samples[0].cpu().numpy()\n",
    "\n",
    "        return sample_image\n",
    "    else:\n",
    "        #### Inpainting directly from the 256x256 images\n",
    "        # TODO: Implement this\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V0JZLKZb_io2"
   },
   "source": [
    "### Gradio Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1741669905164,
     "user": {
      "displayName": "Arnauld LINVANI",
      "userId": "15767780697894810406"
     },
     "user_tz": -60
    },
    "id": "eA9SPYZ7_io2"
   },
   "outputs": [],
   "source": [
    "def create_gradio_interface():\n",
    "    \"\"\"Create the Gradio interface for the inpainting application.\"\"\"\n",
    "    with gr.Blocks(title=\"RePaint with GLIDE Text-Driven Inpainting\") as app:\n",
    "        gr.Markdown(\"# Text-Driven Image Inpainting with RePaint and GLIDE\")\n",
    "        gr.Markdown(\n",
    "            \"Upload an image, draw directly on it to create a mask (white areas will be inpainted), and enter a text prompt.\")\n",
    "\n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                input_image = gr.ImageMask(label=\"Upload & Draw Mask (white areas will be inpainted)\",\n",
    "                                       image_mode=\"RGBA\",\n",
    "                                       type='numpy',\n",
    "                                       sources=('upload'),\t\n",
    "                                       interactive=True)\n",
    "\n",
    "                prompt = gr.Textbox(label=\"Text Prompt\", placeholder=\"Describe what should be in the masked area...\")\n",
    "                guidance_scale = gr.Slider(minimum=1.0, maximum=15.0, value=7.0, step=0.5,\n",
    "                                           label=\"Guidance Scale (higher = more text influence)\")\n",
    "                submit_btn = gr.Button(\"Generate Inpainting\")\n",
    "\n",
    "            with gr.Column():\n",
    "                output_image = gr.Image(label=\"Inpainted Result\")\n",
    "\n",
    "        submit_btn.click(\n",
    "            fn=process_input_with_mask,\n",
    "            inputs=[input_image, prompt, guidance_scale],\n",
    "            outputs=output_image\n",
    "        )\n",
    "\n",
    "        gr.Markdown(\"## How to use\")\n",
    "        gr.Markdown(\"\"\"\n",
    "        1. Upload an image\n",
    "        2. Draw directly on the image with white brush to create a mask (white areas will be inpainted)\n",
    "        3. Enter a text prompt describing what you want in the masked area\n",
    "        4. Click 'Generate Inpainting' and wait for the result\n",
    "        5. Adjust the guidance scale if needed (higher values follow the text more closely)\n",
    "\n",
    "        Tips:\n",
    "        - Use a larger brush radius for covering larger areas\n",
    "        - You can clear your drawing and start over using the clear button\n",
    "        - Be specific in your text prompt for best results\n",
    "        \"\"\")\n",
    "\n",
    "    return app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 589
    },
    "executionInfo": {
     "elapsed": 2190,
     "status": "ok",
     "timestamp": 1741666975381,
     "user": {
      "displayName": "Arnauld LINVANI",
      "userId": "15767780697894810406"
     },
     "user_tz": -60
    },
    "id": "zh6PZ4jU_io2",
    "outputId": "c1d32129-bd8b-4080-ec3d-bb07a24ef437"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://14ff3813002eca503b.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://14ff3813002eca503b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/infres/kbrowder-24/jp/CSC_52002_EP_Generative_AI_Project/glide-text2im-main/glide_text2im/download.py:71: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return th.load(path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating inpainted image...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4570/4570 [03:57<00:00, 19.24it/s]\n",
      "/home/infres/kbrowder-24/jp/CSC_52002_EP_Generative_AI_Project/glide-text2im-main/glide_text2im/download.py:71: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return th.load(path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating inpainted image...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4570/4570 [03:44<00:00, 20.35it/s]\n",
      "/home/infres/kbrowder-24/jp/CSC_52002_EP_Generative_AI_Project/glide-text2im-main/glide_text2im/download.py:71: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return th.load(path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating inpainted image...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 2397/4570 [00:58<00:55, 38.90it/s]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "demo = create_gradio_interface()\n",
    "os.makedirs(\"sample_images\", exist_ok=True)\n",
    "demo.launch(share=True, debug=True, inline=True, show_error=True)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1288da435c5b4459a3e0180990714519": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_491a1267ca054ef1bcb56cc7a9a3a298",
       "IPY_MODEL_9c9bb4139bae4e53a83ed7151aa0b177",
       "IPY_MODEL_1f47b227cb424fa994131a7781f84c1c"
      ],
      "layout": "IPY_MODEL_53681de83b624740a268888cafa2126d"
     }
    },
    "1f47b227cb424fa994131a7781f84c1c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_89e4d8ac577c4cbfafc91d0c6c674011",
      "placeholder": "​",
      "style": "IPY_MODEL_e00a0f6269f942ea8c79387f932d757d",
      "value": " 1.54G/1.54G [09:02&lt;00:00, 3.21MiB/s]"
     }
    },
    "4127d6e619c2441d9eccdb5dbbe2520a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "491a1267ca054ef1bcb56cc7a9a3a298": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8141a783d26c47429923dbe3808f5d25",
      "placeholder": "​",
      "style": "IPY_MODEL_8f8df97d3d794b39ba026aa18e2b927f",
      "value": "100%"
     }
    },
    "53681de83b624740a268888cafa2126d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8141a783d26c47429923dbe3808f5d25": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "89e4d8ac577c4cbfafc91d0c6c674011": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8f8df97d3d794b39ba026aa18e2b927f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9c9bb4139bae4e53a83ed7151aa0b177": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4127d6e619c2441d9eccdb5dbbe2520a",
      "max": 1540394339,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_efa89441255f42fa92f11f551d73975d",
      "value": 1540394339
     }
    },
    "e00a0f6269f942ea8c79387f932d757d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "efa89441255f42fa92f11f551d73975d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
