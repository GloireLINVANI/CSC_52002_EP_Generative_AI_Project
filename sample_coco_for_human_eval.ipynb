{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/infres/kbrowder-24/jp/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import repaint_sampling as RS\n",
    "import repaint_patcher as RP\n",
    "import prepare_glide_inpaint as PGI\n",
    "from image_util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.datasets import ImageFolder, DatasetFolder\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "\n",
    "size = 64\n",
    "large_size = 256\n",
    "\n",
    "common_transform = transforms.Compose([\n",
    "    transforms.Lambda(lambda x: x.convert(\"RGB\")),\n",
    "    transforms.Resize(large_size),\n",
    "    transforms.CenterCrop(large_size),\n",
    "    transforms.Resize((size, size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x * 2 - 1),\n",
    "])\n",
    "with open('data/datasets/imagenet.json') as f:\n",
    "    imagenet_labels = json.load(f)\n",
    "\n",
    "def imagenet_target_transform(target):\n",
    "    return imagenet_labels[int(datasets['imagenet_val'].classes[target])]\n",
    "\n",
    "\n",
    "\n",
    "class CocoFolder(Dataset):\n",
    "    def __init__(self, root, annotations_json, transform=None, target_transform=None):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "        with open(annotations_json) as f:\n",
    "            coco_json = json.load(f)\n",
    "\n",
    "        annotations_by_id = {_ann['image_id']: _ann for _ann in coco_json['annotations']}\n",
    "        coco_filename_to_annotations = {_img['file_name']: annotations_by_id[_img['id']]['caption'] for _img in coco_json['images']}\n",
    "        self.classes = list(set(coco_filename_to_annotations.keys()))\n",
    "        self.classes.sort()\n",
    "\n",
    "        all_files = os.listdir(root)\n",
    "        all_images = [f for f in all_files if f.endswith('.jpg')]\n",
    "        self.image_paths = [os.path.basename(f) for f in all_images]\n",
    "        self.targets = [coco_filename_to_annotations[os.path.basename(f)] for f in all_images]\n",
    "        self.image_paths = [os.path.join(root, f) for f in self.image_paths]\n",
    "        self.samples = list(zip(self.image_paths, self.targets))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, target = self.samples[idx]\n",
    "        img = Image.open(path)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "        return img, target\n",
    "\n",
    "datasets = {\n",
    "    # 'imagenet_val': ImageFolder('data/datasets/ILSVRC2012_img_val_subset', transform=common_transform, target_transform=imagenet_target_transform),\n",
    "    'coco_val2017': CocoFolder('data/datasets/val2017', 'data/annotations/captions_val2017.json', transform=common_transform),\n",
    "    # 'places_365_train': ImageFolder('data/datasets/places365_standard/train', transform=common_transform, target_transform=lambda x: p365t_classes[x].replace(\"_\", \" \")),\n",
    "    # 'places_365_val': ImageFolder('data/datasets/places365_standard/val', transform=common_transform,  target_transform=lambda x: p365v_classes[x].replace(\"_\", \" \")),\n",
    "}\n",
    "\n",
    "# p365t_classes = datasets['places_365_train'].classes\n",
    "# p365v_classes = datasets['places_365_val'].classes\n",
    "\n",
    "masks = {\n",
    "    'ex64': read_mask('data/masks/64/ex64.png', size=64),\n",
    "    'genhalf': read_mask('data/masks/64/genhalf.png',size=64),\n",
    "    'sr64': read_mask('data/masks/64/sr64.png',size=64),\n",
    "    'thick': read_mask('data/masks/64/thick.png',size=64),\n",
    "    'thin': read_mask('data/masks/64/thin.png',size=64),\n",
    "    'vs64': read_mask('data/masks/64/vs64.png',size=64),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_transform_large = transforms.Compose([\n",
    "    transforms.Lambda(lambda x: x.convert(\"RGB\")),\n",
    "    transforms.Resize(large_size),\n",
    "    transforms.CenterCrop(large_size),\n",
    "    transforms.Resize((large_size, large_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x * 2 - 1),\n",
    "])\n",
    "\n",
    "datasets_large = {\n",
    "    # 'imagenet_val': ImageFolder('data/datasets/ILSVRC2012_img_val_subset', transform=common_transform, target_transform=imagenet_target_transform),\n",
    "    'coco_val2017': CocoFolder('data/datasets/val2017', 'data/annotations/captions_val2017.json', transform=common_transform_large),\n",
    "    # 'places_365_train': ImageFolder('data/datasets/places365_standard/train', transform=common_transform, target_transform=lambda x: p365t_classes[x].replace(\"_\", \" \")),\n",
    "    # 'places_365_val': ImageFolder('data/datasets/places365_standard/val', transform=common_transform,  target_transform=lambda x: p365v_classes[x].replace(\"_\", \" \")),\n",
    "}\n",
    "\n",
    "masks_large = {\n",
    "    'ex64': read_mask('data/masks/64/ex64.png', size=256),\n",
    "    'genhalf': read_mask('data/masks/64/genhalf.png',size=256),\n",
    "    'sr64': read_mask('data/masks/64/sr64.png',size=256, resample=Image.NEAREST),\n",
    "    'thick': read_mask('data/masks/64/thick.png',size=256),\n",
    "    'thin': read_mask('data/masks/64/thin.png',size=256),\n",
    "    'vs64': read_mask('data/masks/64/vs64.png',size=256, resample=Image.NEAREST),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "output_dir = 'large_masked_coco'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Function to apply mask to image\n",
    "def apply_mask(image, mask):\n",
    "    return image * mask.squeeze(0)\n",
    "\n",
    "# Iterate over each mask in masks_large\n",
    "for mask_name, mask in masks_large.items():\n",
    "    # Read two random images from coco_val2017 in datasets_large\n",
    "    coco_dataset = datasets_large['coco_val2017']\n",
    "    indices = random.sample(range(len(coco_dataset)), 2)\n",
    "    \n",
    "    for idx in indices:\n",
    "        img, _ = coco_dataset[idx]\n",
    "        \n",
    "        # Apply the mask to the image\n",
    "        masked_img = apply_mask(img, mask)\n",
    "        \n",
    "        # Save both the original image and the masked one\n",
    "        original_img_path = os.path.join(output_dir, f'{mask_name}_original_{idx}.png')\n",
    "        masked_img_path = os.path.join(output_dir, f'{mask_name}_masked_{idx}.png')\n",
    "        \n",
    "        F.to_pil_image((img + 1) / 2).save(original_img_path)\n",
    "        _, target = coco_dataset[idx]\n",
    "        target_label = target.replace(\" \", \"_\")\n",
    "        masked_img_path = os.path.join(output_dir, f'{mask_name}_masked_{idx}_{target_label}.png')\n",
    "        F.to_pil_image((masked_img + 1) / 2).save(masked_img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
